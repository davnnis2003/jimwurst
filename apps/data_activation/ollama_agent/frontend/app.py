
import streamlit as st
import os

import sys
import subprocess
import signal

# Add project root to sys.path to allow imports
current_dir = os.path.dirname(os.path.abspath(__file__))
# frontend -> ollama_agent -> data_activation -> apps -> root (4 levels up)
project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

from apps.data_activation.ollama_agent.backend.agent import JimwurstAgent

st.set_page_config(
    page_title="Jimwurst AI",
    page_icon="ðŸŒ­",
    layout="wide"
)

st.title("ðŸŒ­ Jimwurst AI")

# Sidebar
with st.sidebar:
    st.header("Settings")
    model_name = st.text_input("Ollama Model", value="qwen2.5:3b")
    st.markdown(
        "Ensure **Ollama** is running locally "
        "(open the Ollama app or run 'ollama serve' in a terminal)."
    )
    if st.button("Check Connection"):
        # Temporary agent to check connection
        temp_agent = JimwurstAgent(model_name=model_name)
        if temp_agent.check_ollama_connection():
            st.success("Connected to Ollama!")
        else:
            st.error("Could not connect to Ollama.")

    st.markdown("---")
    if st.button("Shutdown System", type="primary"):
        with st.spinner("Shutting down services..."):
            try:
                # Run 'make down' to stop Docker containers
                subprocess.run(["make", "down"], check=True, cwd=project_root)
                st.success("Services stopped. Exiting application...")
                # Kill the current Streamlit process
                os.kill(os.getpid(), signal.SIGTERM)
            except Exception as e:
                st.error(f"Error during shutdown: {e}")



# Initialize Session State
if "messages" not in st.session_state:
    st.session_state.messages = []

if "agent" not in st.session_state:
    st.session_state.agent = JimwurstAgent(model_name=model_name)

# Display Chat History
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar="ðŸŒ­" if message["role"] == "assistant" else "ðŸ‘¤"):
        st.markdown(message["content"])

# --- New Feature: Showcase & Upload ---

# Variable to hold the user's next action/prompt
next_prompt = None

with st.container():
    # 1. Feature Showcase Bubbles
    st.caption("What would you like to do?")
    col1, col2, col3 = st.columns(3)
    
    if col1.button("ðŸ“‚ Upload & Transform", use_container_width=True):
        # We can't auto-open the uploader, but we can set a prompt to guide or just rely on the uploader below
        # For now, let's treat this as a prompt query or a guide
        next_prompt = "I would like to upload a CSV file and run transformations."
        
    if col2.button("ðŸ’¾ Show DWH Data", use_container_width=True):
        next_prompt = "Show me the data currently in the DWH."
        
    if col3.button("ðŸ“Š Provide Insights", use_container_width=True):
        next_prompt = "Provide insights from the existing data."

# File upload + source selection panel (always visible above chat input)
with st.container(border=True):
    st.markdown("**Attach a CSV or Excel file to this chat (drag & drop or browse):**")
    uploaded_file = st.file_uploader(
        "Drag & drop a CSV or Excel file here or click to browse",
        type=["csv", "xlsx", "xls"],
        accept_multiple_files=False,
        key="chat_file_uploader",
    )

    if uploaded_file is not None:
        # Persist the uploaded file
        user_home = os.path.expanduser("~")
        jimwurst_data_dir = os.path.join(user_home, ".jimwurst_data")
        os.makedirs(jimwurst_data_dir, exist_ok=True)

        file_path = os.path.join(jimwurst_data_dir, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getvalue())

        st.session_state.last_uploaded_file_path = file_path
        st.session_state.last_uploaded_file_name = uploaded_file.name

        st.success(f"Received file: {uploaded_file.name}")

        st.markdown("**What application generated this data?**")
        source_choice = st.radio(
            "Choose one of the options below:",
            [
                "Substack (Manual Export)",
                "Linkedin (Simple Export)",
                "Linkedin (Full Export)",
                "Others (Please specify)",
            ],
            key="data_source_choice",
        )

        other_source = ""
        if source_choice == "Others (Please specify)":
            other_source = st.text_input(
                "Please specify the application",
                key="data_source_other",
            )

        if st.button("Send file details to agent", key="confirm_file_source"):
            if source_choice == "Others (Please specify)" and not other_source:
                st.warning("Please specify the application before continuing.")
            else:
                source_app = (
                    other_source if source_choice == "Others (Please specify)" else source_choice
                )
                # Use the last saved file path so we can safely reference it across reruns
                file_path = st.session_state.get("last_uploaded_file_path", file_path)
                next_prompt = (
                    f"Please ingest the data from this file: {file_path}. "
                    f"This data was generated by {source_app}. After ingestion, please run the transformations."
                )


# User Input (Standard Chat) â€“ pinned at bottom by Streamlit
chat_input_prompt = st.chat_input("How can I help you today?")

# Determine final prompt source
if chat_input_prompt:
    next_prompt = chat_input_prompt

# Process the prompt if one exists (either from buttons, upload, or chat input)
if next_prompt:
    # Add user message to state
    st.session_state.messages.append({"role": "user", "content": next_prompt})
    with st.chat_message("user", avatar="ðŸ‘¤"):
        st.markdown(next_prompt)


    # Generate Response
    with st.chat_message("assistant", avatar="ðŸŒ­"):
        # Create a status container for the thinking process (Perplexity-style)
        with st.status("ðŸ§  Agent Thinking Process...", expanded=False) as status:
            # Use a container with fixed height to enable scrolling
            # This will automatically stick to the bottom as new content is added
            with st.container(height=300, border=False):
                thoughts_placeholder = st.empty()
            
            # Ensure agent model matches sidebar
            if st.session_state.agent.model_name != model_name:
                 st.session_state.agent = JimwurstAgent(model_name=model_name)
            
            try:
                # Import our custom callback
                from utils.streamlit_callback import StreamlitThinkingCallback
                
                # Create callback handler pointing to the placeholder
                st_callback = StreamlitThinkingCallback(thoughts_placeholder)
                
                # Call the agent with the callback
                response, intermediate_steps = st.session_state.agent.chat(next_prompt, callbacks=[st_callback])
                
                # If we got response but callback didn't run (unlikely but possible), show something
                if not st_callback.text:
                    thoughts_placeholder.markdown("*Processed without detailed steps*")
                
                # Mark status as complete
                status.update(label="ðŸ§  Agent Thinking Process - Complete", state="complete", expanded=False)
                
            except Exception as e:
                response = f"Error: {str(e)}"
                thoughts_placeholder.markdown(f"\n\n**Error occurred:** {str(e)}")
                status.update(label="ðŸ§  Agent Thinking Process - Error", state="error", expanded=True)
        
        # Display the final answer outside the status box
        st.markdown(f"**Answer:**\n\n{response}")
    
    # Add assistant message to state
    st.session_state.messages.append({"role": "assistant", "content": response})

