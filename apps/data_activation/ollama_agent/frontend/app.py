
import streamlit as st
import os

import sys
import subprocess
import signal

# Add project root to sys.path to allow imports
current_dir = os.path.dirname(os.path.abspath(__file__))
# frontend -> ollama_agent -> data_activation -> apps -> root (4 levels up)
project_root = os.path.abspath(os.path.join(current_dir, "../../../.."))
if project_root not in sys.path:
    sys.path.append(project_root)

from apps.data_activation.ollama_agent.backend.agent import JimwurstAgent

st.set_page_config(
    page_title="Jimwurst AI",
    page_icon="ðŸŒ­",
    layout="wide"
)

st.title("ðŸŒ­ Jimwurst AI")

# Sidebar
with st.sidebar:
    st.header("Settings")
    model_name = st.text_input("Ollama Model", value="qwen2.5:3b")
    st.markdown("Ensure **Ollama** is running locally.")
    if st.button("Check Connection"):
        # Temporary agent to check connection
        temp_agent = JimwurstAgent(model_name=model_name)
        if temp_agent.check_ollama_connection():
            st.success("Connected to Ollama!")
        else:
            st.error("Could not connect to Ollama.")

    st.markdown("---")
    if st.button("Shutdown System", type="primary"):
        with st.spinner("Shutting down services..."):
            try:
                # Run 'make down' to stop Docker containers
                subprocess.run(["make", "down"], check=True, cwd=project_root)
                st.success("Services stopped. Exiting application...")
                # Kill the current Streamlit process
                os.kill(os.getpid(), signal.SIGTERM)
            except Exception as e:
                st.error(f"Error during shutdown: {e}")



# Initialize Session State
if "messages" not in st.session_state:
    st.session_state.messages = []

if "agent" not in st.session_state:
    st.session_state.agent = JimwurstAgent(model_name=model_name)

# Display Chat History
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar="ðŸŒ­" if message["role"] == "assistant" else "ðŸ‘¤"):
        st.markdown(message["content"])

# --- New Feature: Showcase & Upload ---

# Variable to hold the user's next action/prompt
next_prompt = None

with st.container():
    # 1. Feature Showcase Bubbles
    st.caption("What would you like to do?")
    col1, col2, col3 = st.columns(3)
    
    if col1.button("ðŸ“‚ Upload & Transform", use_container_width=True):
        # We can't auto-open the uploader, but we can set a prompt to guide or just rely on the uploader below
        # For now, let's treat this as a prompt query or a guide
        next_prompt = "I would like to upload a CSV file and run transformations."
        
    if col2.button("ðŸ’¾ Show DWH Data", use_container_width=True):
        next_prompt = "Show me the data currently in the DWH."
        
    if col3.button("ðŸ“Š Provide Insights", use_container_width=True):
        next_prompt = "Provide insights from the existing data."

    # 2. CSV Upload Widget
    with st.expander("Upload a CSV File", expanded=False):
        uploaded_file = st.file_uploader("Choose a CSV file", type="csv")
        source_app = st.text_input("Source Application", placeholder="e.g., Spotify, Apple Health")
        
        if uploaded_file is not None and source_app:
            if st.button("Process Uploaded File"):
                # Define persistent storage path
                user_home = os.path.expanduser("~")
                jimwurst_data_dir = os.path.join(user_home, ".jimwurst_data")
                os.makedirs(jimwurst_data_dir, exist_ok=True)
                
                # Save to persistent file
                file_path = os.path.join(jimwurst_data_dir, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())
                
                # Construct a prompt for the agent to ingest this specific file with context
                next_prompt = f"Please ingest the data from this file: {file_path}. This data was generated by {source_app}. After ingestion, please run the transformations."
                st.success(f"File saved to {file_path}. Sending to agent...")
        elif uploaded_file is not None and not source_app:
            st.warning("Please specify the Source Application before processing.")

# User Input (Standard Chat)
chat_input_prompt = st.chat_input("How can I help you today?")

# Determine final prompt source
if chat_input_prompt:
    next_prompt = chat_input_prompt

# Process the prompt if one exists (either from buttons, upload, or chat input)
if next_prompt:
    # Add user message to state
    st.session_state.messages.append({"role": "user", "content": next_prompt})
    with st.chat_message("user", avatar="ðŸ‘¤"):
        st.markdown(next_prompt)


    # Generate Response
    with st.chat_message("assistant", avatar="ðŸŒ­"):
        # Create a container for streaming thoughts
        thinking_container = st.container()
        response_container = st.empty()
        
        with thinking_container:
            with st.expander("ðŸ§  Agent Thinking Process", expanded=True):
                thoughts_placeholder = st.empty()
                
                # We'll capture the verbose output by modifying the agent's chat method
                # For now, let's show a status
                thoughts_text = "**Starting agent...**\n\n"
                thoughts_placeholder.markdown(thoughts_text)
                
                # Ensure agent model matches sidebar
                if st.session_state.agent.model_name != model_name:
                     st.session_state.agent = JimwurstAgent(model_name=model_name)
                
                try:
                    # Call the agent - capture both stdout and stderr
                    import io
                    import contextlib
                    
                    # Capture stdout and stderr
                    stdout_capture = io.StringIO()
                    stderr_capture = io.StringIO()
                    
                    with contextlib.redirect_stdout(stdout_capture), contextlib.redirect_stderr(stderr_capture):
                        response = st.session_state.agent.chat(next_prompt)
                    
                    # Get captured output
                    captured_stdout = stdout_capture.getvalue()
                    captured_stderr = stderr_capture.getvalue()
                    
                    # Combine outputs
                    captured_output = captured_stdout + captured_stderr
                    
                    if captured_output.strip():
                        thoughts_text += f"```\n{captured_output}\n```"
                    else:
                        thoughts_text += "*No verbose output captured - check console logs*\n"
                    
                    thoughts_placeholder.markdown(thoughts_text)
                    
                except Exception as e:
                    response = f"Error: {str(e)}"
                    thoughts_text += f"\n\n**Error occurred:** {str(e)}"
                    thoughts_placeholder.markdown(thoughts_text)
        
        response_container.markdown(f"**Answer:**\n\n{response}")
    
    # Add assistant message to state
    st.session_state.messages.append({"role": "assistant", "content": response})

